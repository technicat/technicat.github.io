---
layout: post
title: 'The History of Large-Scale Data Pipelines (Part 1): The Migration to Hadoop'
date: '2015-02-20T21:32:28-08:00'
tags: []
tumblr_url: https://fugugames.tumblr.com/post/111617600856/the-history-of-large-scale-data-pipelines-part
---
<a href="http://yahoodevelopers.tumblr.com/post/111616833813/the-history-of-large-scale-data-pipelines-part">The History of Large-Scale Data Pipelines (Part 1): The Migration to Hadoop</a><br/><blockquote class="link_og_blockquote">yahoohadoop: By Matthew Ahrens, Sr. Manager, Advertising Data Systems

  

 Introduction To Data Pipelines

 At Yahoo we build large-scale data pipelines to support many of our key businesses and products.  Data pipelines help to transform, organize, and aggregate key data sets for our advertising businesses, our internal analytics, and many science use cases.  Our pipelines are processing billions of events and many terabytes a day.  

 The data pipelines at Yahoo in general have the following components:

 
 transformation of events that are collected from data centers around the globe

 decoration of data with dimension data coming from user and business systems

 joining of events with secondary events (e.g. joining a user’s click with the user’s original page view)


 validation of data to determine if the activity is human or spam and also to determine if data contains all the proper information to be classified as a storable transaction for analytics and revenue counting


 aggregated data sets for specific internal and external data consumers


 data warehouse system for accessing data sets internal to the pipeline and for data consumers



 As data demands have grown, so has the need to scale out our data pipelines. User and advertising traffic has grown 10x over the past few years, and new types of interactions and instrumentation capabilities have been introduced in the end-to-end system.  As Yahoo has rolled out next products like digital magazines and mobile apps, data pipelines have served as the bridge between the user traffic and the insights needed to help improve the products for Yahoo’s users. 

  

 Data Pipelines Before Hadoop

 As Yahoo built pipelines to support its search and display advertising businesses, the data systems developed internal software to handle billions of transactions a day.  This software was built out of software written in Perl and C++ to handle the components listed above.

 

 For deployment, we built out specific clusters for each type of processing.  For example, there was a dedicated set of servers to do the transformation stage in the data pipeline while another cluster of servers did the event joins.  The dedicated clusters were tuned (in memory and CPU configuration) to be optimized for the type of job performed.

 For data storage, the data pipeline stored data in a custom data warehouse built internally across filers using NFS mounted file systems.  The use of storage filers allowed the data warehouse to store on the order of 100 TB of data for our data pipelines.  The warehouse had client interfaces on the command line, in Perl, and in C++.  Most data consumers did not access the warehouse directly but instead used a proxy server to stream data files from the warehouse to their own processing system.

 Additionally, we have a custom-built scheduling solution to represent the pipeline stage dependencies and execute the workflow for the various stages. There also was monitoring components for various parts of the pipeline. 

  

 Migrating to Hadoop

 In 2009, the business demands on our data pipelines were growing.  External customers wanted more access to event-level data and our pipelines were hitting scale issues on the custom systems.  The forecast was for a 3-5x increase in the amount of data, but our existing systems had only 10-20% headroom.  In addition, the amount of overhead needed to manage the hardware allocation for our custom clusters and data warehouse became a growing concern. 

 Hadoop was created in 2006 and by late 2009 it was ready for large-scale production use in Yahoo as large multi-tenant clusters became available with petabytes of storage and thousands of CPUs.  The data pipeline team made the decision to start the migration to Hadoop to be the place where our data pipeline processing and storage would live.  One of the big advantages to be on Hadoop was that the servers in the Hadoop multi-tenant cluster could serve the purpose of storage and compute resources.  That helped free our engineers from having to do hardware management across our dedicated clusters and data warehouse that we had previously done with our legacy systems.

 It took a small team of engineers around six months to fully migrate an existing data pipeline from the legacy systems to Hadoop.  The team chose Pig Latin as the main scripting language for our data pipeline stages along with a few Map Reduce Java jobs where performance optimization was needed.  The data validation across the legacy and Hadoop pipeline was arduous, but we eventually enabled live customers in early 2010.

 The first customers who migrated saw an immediate reduction in data latency; the data sets that took ~6 hours on the legacy system path now were available in less than 4 hours from the Hadoop system.  In addition, once we had the raw event-level data on Hadoop a number of internal teams surfaced that wanted to access the data.  Because the Hadoop interface (having data stored in HDFS) was a simpler access mechanism, the amount of internal data users grew exponentially. 

 

 As more legacy customers move to Hadoop later in
2010, we faced some significant obstacles.
Some of the legacy data streams were available in around an hour, and
that type of latency was a challenge in 2010 given where the Hadoop platform
was it in terms of performance and maturity.
In addition, the consequence of being on a multi-tenant cluster was that
other users of Hadoop could have an adverse affect on the data pipeline’s
performance and stability.

 After many joint sessions between our data
pipeline and Hadoop development and operations teams, we were able to make
significant improvements both in the data pipelines jobs and the Hadoop
platform, yielding a latency improvement of around an hour for most data sets.  By mid-2011, the data pipelines on Hadoop
were considered fully stable and could handle the demands of batch systems for
our user and advertising businesses.
Over time, the Hadoop pipelines have adopted other frameworks such as Hive,
Oozie, and HBase for various use cases inside our pipelines.  Migrating to Oozie allowed us to retire our
custom-built scheduler and integrate scheduling on Hadoop to make monitoring
and job execution run on the same platform.

 In the time since we migrated to Hadoop, our data
latency and job run times have continued to shrink even as our data volume has
grown.  Critical data sets that provide a
feedback loop to our ad servers have reduced latency from 90 minutes to 45
minutes.  Complex join jobs that run
every 5 minutes had reduced in runtime from 30 minutes to less than 10 minutes
over the past few years.  In addition,
the redundancy and availability of the Hadoop platform has allowed us to not
have any data outages for the revenue systems that run on Hadoop.  This is a great testament to the maturity of
the Hadoop platform and its ability to scale even as more applications across
Yahoo migrated to Hadoop and started using the multi-tenant clusters.  

 

 The Rise of
Real Time

 Coming in the next part of the history of
large-scale data pipelines is the introduction of real-time data pipelines for
Yahoo businesses and how Hadoop pipelines have evolved to complement real-time
systems.</blockquote>
